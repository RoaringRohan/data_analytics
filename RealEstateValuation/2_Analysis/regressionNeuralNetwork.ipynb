{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d9be6c-6467-41d1-a982-ba1d8abddbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23958985-d4b2-44d5-ac97-6737ea49c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_excel('../1_Data/Real estate valuation data set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f0c448-08b8-4e1f-baf2-ef59bd1e8655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming data\n",
    "X = data.iloc[:, 2:-1].values  # Selecting feature columns\n",
    "y = data.iloc[:, -1].values.reshape(-1, 1)  # Target column (house price)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y = scaler_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c0c52e-678d-4c90-94ef-5cd01299c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d319efd-f547-4019-876f-73ab5a518ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Architecture\n",
    "input_size = X.shape[1]\n",
    "hidden_sizes = [64, 32, 16]\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05941418-7ad7-46c2-b5ea-5846247598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for network architecture\n",
    "\n",
    "# This function initializes parameters by producing randomized weights for each layer and setting bias to zero\n",
    "# Takes the array of how many neurons would be in each layer and outputs a dictionary for the weights and biases\n",
    "def initialize_parameters():\n",
    "    np.random.seed(42)\n",
    "    parameters = {\n",
    "        'W1': np.random.randn(input_size, hidden_sizes[0]) * np.sqrt(2 / input_size),\n",
    "        'b1': np.zeros((1, hidden_sizes[0])),\n",
    "        'W2': np.random.randn(hidden_sizes[0], hidden_sizes[1]) * np.sqrt(2 / hidden_sizes[0]),\n",
    "        'b2': np.zeros((1, hidden_sizes[1])),\n",
    "        'W3': np.random.randn(hidden_sizes[1], hidden_sizes[2]) * np.sqrt(2 / hidden_sizes[1]),\n",
    "        'b3': np.zeros((1, hidden_sizes[2])),\n",
    "        'W4': np.random.randn(hidden_sizes[2], output_size) * np.sqrt(2 / hidden_sizes[2]),\n",
    "        'b4': np.zeros((1, output_size))\n",
    "    }\n",
    "    return parameters\n",
    "\n",
    "# Activation function\n",
    "def relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "# Used for adjusting weights during back-propagation\n",
    "def relu_derivative(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)\n",
    "\n",
    "# Forward propagation\n",
    "# Passes a sample through the network, does not change any weights in this function\n",
    "# Requires the sample (which is an array of one value for each feature) as well as the dictionary originally set in the initialization (with the weights/biases)\n",
    "def forward_propagation(X, parameters):\n",
    "    cache = {}\n",
    "    cache['Z1'] = np.dot(X, parameters['W1']) + parameters['b1']\n",
    "    cache['A1'] = relu(cache['Z1'])\n",
    "    \n",
    "    cache['Z2'] = np.dot(cache['A1'], parameters['W2']) + parameters['b2']\n",
    "    cache['A2'] = relu(cache['Z2'])\n",
    "    \n",
    "    cache['Z3'] = np.dot(cache['A2'], parameters['W3']) + parameters['b3']\n",
    "    cache['A3'] = relu(cache['Z3'])\n",
    "    \n",
    "    cache['Z4'] = np.dot(cache['A3'], parameters['W4']) + parameters['b4']\n",
    "    cache['A4'] = cache['Z4']  # Linear activation for regression output\n",
    "    \n",
    "    return cache\n",
    "\n",
    "# Compute loss (MSE)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    return (1 / (2 * m)) * np.sum((y_pred - y_true) ** 2)\n",
    "\n",
    "# Backward propagation\n",
    "# For finding the adjusted weights based on loss for that particular sample\n",
    "def backward_propagation(X, y, parameters, cache):\n",
    "    m = y.shape[0]\n",
    "    grads = {}\n",
    "    \n",
    "    dZ4 = cache['A4'] - y\n",
    "    grads['dW4'] = (1 / m) * np.dot(cache['A3'].T, dZ4)\n",
    "    grads['db4'] = (1 / m) * np.sum(dZ4, axis=0, keepdims=True)\n",
    "    \n",
    "    dA3 = np.dot(dZ4, parameters['W4'].T)\n",
    "    dZ3 = dA3 * relu_derivative(cache['Z3'])\n",
    "    grads['dW3'] = (1 / m) * np.dot(cache['A2'].T, dZ3)\n",
    "    grads['db3'] = (1 / m) * np.sum(dZ3, axis=0, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(dZ3, parameters['W3'].T)\n",
    "    dZ2 = dA2 * relu_derivative(cache['Z2'])\n",
    "    grads['dW2'] = (1 / m) * np.dot(cache['A1'].T, dZ2)\n",
    "    grads['db2'] = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(dZ2, parameters['W2'].T)\n",
    "    dZ1 = dA1 * relu_derivative(cache['Z1'])\n",
    "    grads['dW1'] = (1 / m) * np.dot(X.T, dZ1)\n",
    "    grads['db1'] = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "    \n",
    "    return grads\n",
    "\n",
    "# Update parameters with SGDM\n",
    "def update_parameters(parameters, grads, velocities, learning_rate, momentum=0.9):\n",
    "    for key in parameters.keys():\n",
    "        if key.startswith('W') or key.startswith('b'):\n",
    "            velocities[key] = momentum * velocities[key] - learning_rate * grads['d' + key]\n",
    "            parameters[key] += velocities[key]\n",
    "    return parameters, velocities\n",
    "\n",
    "# Training the network\n",
    "def train_network(X_train, y_train, epochs=1000, learning_rate=0.05):\n",
    "    parameters = initialize_parameters()\n",
    "    velocities = {key: np.zeros_like(value) for key, value in parameters.items()}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        cache = forward_propagation(X_train, parameters)\n",
    "        loss = compute_loss(y_train, cache['A4'])\n",
    "        grads = backward_propagation(X_train, y_train, parameters, cache)\n",
    "        parameters, velocities = update_parameters(parameters, grads, velocities, learning_rate)\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "    return parameters\n",
    "\n",
    "# Evaluate the network\n",
    "def evaluate_network(X_test, y_test, parameters):\n",
    "    cache = forward_propagation(X_test, parameters)\n",
    "    y_pred = cache['A4']\n",
    "    \n",
    "    ss_residual = np.sum((y_test - y_pred) ** 2)\n",
    "    ss_total = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "    r2_score = 1 - (ss_residual / ss_total)\n",
    "    \n",
    "    mse = compute_loss(y_test, y_pred)\n",
    "    return mse, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6c9aa0-57c0-4166-b1c2-3ab170cb4acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.4580\n",
      "Epoch 200/1000, Loss: 0.1956\n",
      "Epoch 300/1000, Loss: 0.1814\n",
      "Epoch 400/1000, Loss: 0.1735\n",
      "Epoch 500/1000, Loss: 0.1687\n",
      "Epoch 600/1000, Loss: 0.1655\n",
      "Epoch 700/1000, Loss: 0.1629\n",
      "Epoch 800/1000, Loss: 0.1606\n",
      "Epoch 900/1000, Loss: 0.1584\n",
      "Epoch 1000/1000, Loss: 0.1562\n",
      "Test Mean Squared Error: 0.1044\n",
      "R² Score: 0.7701\n",
      "   Actual Price  Predicted Price\n",
      "0          45.1        52.163421\n",
      "1          42.3        36.901442\n",
      "2          52.2        53.178658\n",
      "3          37.3        46.712917\n",
      "4          22.8        23.220962\n",
      "5          36.3        41.909712\n",
      "6          53.0        48.373440\n",
      "7          51.4        48.160268\n",
      "8          16.1        19.694691\n",
      "9          59.0        54.242535\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "parameters = train_network(X_train, y_train, epochs=1000, learning_rate=0.05)\n",
    "\n",
    "# Evaluate the model\n",
    "mse, r2_score = evaluate_network(X_test, y_test, parameters)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R² Score: {r2_score:.4f}\")\n",
    "\n",
    "# Get actual prices to compare with\n",
    "# This required reversing the scaling\n",
    "y_pred_scaled = forward_propagation(X_test, parameters)['A4']\n",
    "y_pred_actual = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Display samples and their predicted and actual prices\n",
    "results = pd.DataFrame({'Actual Price': y_test_actual.flatten(), 'Predicted Price': y_pred_actual.flatten()})\n",
    "print(results.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
